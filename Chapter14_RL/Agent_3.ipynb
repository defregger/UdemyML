{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size # 4\n",
    "        self.action_size = action_size # 2\n",
    "        self.memory = []\n",
    "        self.gamma = 0.97 # discount rate\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_lower_bound = 0.05 # lower bound\n",
    "        self.epsilon_decay = 0.999 # decay rate\n",
    "        self.lr = 0.0001 # learning rate for the dnn\n",
    "        self.model = self.build_model() # keras dnn model\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(24))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(lr=self.lr))\n",
    "        #model.summary()\n",
    "        return model\n",
    "    \n",
    "    # We save the prev. experiences for later re-training\n",
    "    def remember(self, state, action, reward, next_state, is_finished):\n",
    "        self.memory.append([state, action, reward, next_state, is_finished])\n",
    "        \n",
    "    # Get the action from the DNN or by Random Sampling \n",
    "    def get_action(self, state):\n",
    "        # Select a random action\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        # Compute the action from the DNN\n",
    "        action = self.model.predict(state)[0] # [0.25, 0.7]\n",
    "        return np.argmax(action)\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, is_finished in minibatch:\n",
    "            target = reward\n",
    "            if not is_finished:\n",
    "                # Q-learning reward formula\n",
    "                target = (reward + self.gamma * np.max(self.model.predict(next_state)))\n",
    "            # Train the agent to approx. the current state to future rewards\n",
    "            target_future = self.model.predict(state) # [0.25, 0.7]\n",
    "            target_future[0][action] = target\n",
    "            self.model.fit(state, target_future, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_lower_bound:\n",
    "            self.epsilon = self.epsilon * self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01736505, -0.00308276, -0.02204238,  0.03357288])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes = 1000\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 4\n",
    "action_size = 2\n",
    "agent = Agent(state_size, action_size)\n",
    "\n",
    "is_finished = False\n",
    "episodes = 1000\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0  Time-Score:  106  Epsilon:  0.04953625663766238\n",
      "Episode:  1  Time-Score:  89  Epsilon:  0.04953625663766238\n",
      "Episode:  2  Time-Score:  73  Epsilon:  0.04953625663766238\n",
      "Episode:  3  Time-Score:  87  Epsilon:  0.04953625663766238\n",
      "Episode:  4  Time-Score:  77  Epsilon:  0.04953625663766238\n",
      "Episode:  5  Time-Score:  83  Epsilon:  0.04953625663766238\n",
      "Episode:  6  Time-Score:  97  Epsilon:  0.04953625663766238\n",
      "Episode:  7  Time-Score:  85  Epsilon:  0.04953625663766238\n",
      "Episode:  8  Time-Score:  90  Epsilon:  0.04953625663766238\n",
      "Episode:  9  Time-Score:  145  Epsilon:  0.04953625663766238\n",
      "Episode:  10  Time-Score:  99  Epsilon:  0.04953625663766238\n",
      "Episode:  11  Time-Score:  179  Epsilon:  0.04953625663766238\n",
      "Episode:  12  Time-Score:  82  Epsilon:  0.04953625663766238\n",
      "Episode:  13  Time-Score:  93  Epsilon:  0.04953625663766238\n",
      "Episode:  14  Time-Score:  283  Epsilon:  0.04953625663766238\n",
      "Episode:  15  Time-Score:  71  Epsilon:  0.04953625663766238\n",
      "Episode:  16  Time-Score:  204  Epsilon:  0.04953625663766238\n",
      "Episode:  17  Time-Score:  196  Epsilon:  0.04953625663766238\n",
      "Episode:  18  Time-Score:  254  Epsilon:  0.04953625663766238\n",
      "Episode:  19  Time-Score:  98  Epsilon:  0.04953625663766238\n",
      "Episode:  20  Time-Score:  73  Epsilon:  0.04953625663766238\n",
      "Episode:  21  Time-Score:  105  Epsilon:  0.04953625663766238\n",
      "Episode:  22  Time-Score:  188  Epsilon:  0.04953625663766238\n",
      "Episode:  23  Time-Score:  163  Epsilon:  0.04953625663766238\n",
      "Episode:  24  Time-Score:  120  Epsilon:  0.04953625663766238\n",
      "Episode:  25  Time-Score:  111  Epsilon:  0.04953625663766238\n",
      "Episode:  26  Time-Score:  282  Epsilon:  0.04953625663766238\n",
      "Episode:  27  Time-Score:  105  Epsilon:  0.04953625663766238\n",
      "Episode:  28  Time-Score:  111  Epsilon:  0.04953625663766238\n",
      "Episode:  29  Time-Score:  109  Epsilon:  0.04953625663766238\n",
      "Episode:  30  Time-Score:  105  Epsilon:  0.04953625663766238\n",
      "Episode:  31  Time-Score:  151  Epsilon:  0.04953625663766238\n",
      "Episode:  32  Time-Score:  202  Epsilon:  0.04953625663766238\n",
      "Episode:  33  Time-Score:  135  Epsilon:  0.04953625663766238\n",
      "Episode:  34  Time-Score:  442  Epsilon:  0.04953625663766238\n",
      "Episode:  35  Time-Score:  220  Epsilon:  0.04953625663766238\n",
      "Episode:  36  Time-Score:  415  Epsilon:  0.04953625663766238\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for frame in range(500):\n",
    "        # Get random/dnn action\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, is_finished, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        if is_finished == True:\n",
    "            reward = -1\n",
    "        # Save the epsiode\n",
    "        agent.remember(state, action, reward, next_state, is_finished)\n",
    "        state = next_state\n",
    "        if is_finished == True:\n",
    "            print(\"Episode: \", episode, \" Time-Score: \", frame, \" Epsilon: \", agent.epsilon)\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}